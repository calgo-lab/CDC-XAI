{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397c7aa-47da-4d9a-8e43-c6aa9914d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmlb import fetch_data\n",
    "from tab_err.api import MidLevelConfig, mid_level\n",
    "from tab_err import ErrorModel, error_mechanism, error_type\n",
    "from tab_err.error_mechanism import ECAR, ENAR\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from conformal_data_cleaning.cleaner.conformal import ConformalAutoGluonCleaner \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autogluon.eda.auto as auto\n",
    "import autogluon.eda.analysis as eda\n",
    "import autogluon.eda.visualization as viz\n",
    "\n",
    "\n",
    "\n",
    "def run_exp(Dataframe, target_columns, condition_columns, categorical_columns, regression_columns, error_type, error_rate, frac, rows_numbers_xai, diagramm_xai_rows, confidence_level, time_limit, top_k, test_size, reuse_intermediate):\n",
    "    # Dataframe = 'adult'\n",
    "    # target_columns = ['age', 'occupation', 'fnlwgt']\n",
    "    # condition_columns = ['education-num', 'workclass', 'sex']\n",
    "    # categorical_columns = ['occupation']\n",
    "    # regression_columns = ['age', 'fnlwgt']\n",
    "    # error_type = [error_type.Outlier(), error_type.CategorySwap(), error_type.MissingValue()]\n",
    "    # error_rate = 0.2\n",
    "    # frac = 0.07 # Precent of rows that will be used in the CDC expirement\n",
    "    # rows_numbers_xai = 50 #number of rows to be explained in XAI section\n",
    "\n",
    "    cleaned_data = fetch_data(Dataframe)\n",
    "    cleaned_data= cleaned_data.sample(frac=frac, random_state=42)\n",
    "    for cat in categorical_columns:\n",
    "        cleaned_data[cat] = cleaned_data[cat].astype('category') \n",
    "    for num in regression_columns:\n",
    "        cleaned_data[num] = cleaned_data[num].astype('int') \n",
    "\n",
    "    X = cleaned_data.drop(target_columns, axis=1)\n",
    "    y = cleaned_data[target_columns]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    test_data = X_test.copy()\n",
    "    test_data[target_columns] = y_test.copy()\n",
    "    corrupted = test_data.copy()\n",
    "    \n",
    "    train_data = X_train.copy()\n",
    "    train_data[target_columns] = y_train.copy()\n",
    "    \n",
    "    corrupted_data, error_mask = create_errors(corrupted, target_columns, condition_columns, error_type, error_rate)\n",
    "    corrupted_test_copy = corrupted_data.copy()  ## this one will be used in the detection experiment \n",
    "    \n",
    "    #creating the masks, that we will use later in Detection\n",
    "    mask_error = error_mask.copy()\n",
    "    mask_error = mask_error[target_columns]\n",
    "    masks = mask_error.astype(int)\n",
    "    \n",
    "    mask_targets = [] # mask labels such as age-mask and so on, for detection and xai parts\n",
    "    \n",
    "    # these data we are gonna use in transform in the detection section and also for predection in XAI \n",
    "    for target in target_columns:\n",
    "        mask_targets.append(target + '-mask')\n",
    "        corrupted_test_copy[target + '-mask']=np.nan\n",
    "\n",
    "\n",
    "    export_files(corrupted_data,error_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ###### Cleaning data ######\n",
    "\n",
    "    #Using CDC Conformal data cleaning based on Jäger & Bießmann >> https://proceedings.mlr.press/v238/jager24a\n",
    "    #The implementation is from: 1. https://github.com/se-jaeger/conformal-inference/tree/main/conformal_inference\n",
    "                                #2. https://github.com/se-jaeger/conformal-data-cleaning/tree/main/conformal_data_cleaning\n",
    "    \n",
    "    \n",
    "\n",
    "    # in Cleaner we are calling the class ConformalAutoGluonCleaner from the gitHub conformal-data-cleaning, \n",
    "    # we defined a parameter confidence_level in the class and decalred it with a value between 0 and 1\n",
    "    \n",
    "    cleaner = ConformalAutoGluonCleaner(confidence_level=confidence_level,seed=42)\n",
    "    \n",
    "    \n",
    "#during the training, an ML model will be fitted for each column (target_column), where all other columns are\n",
    "#the model’s features, and calibrate its output\n",
    "# calibration_size = 1000 / data.shape[0] in our case almost 10,000 data which means we will use 0.1 of calibration_size, \n",
    "#which will be used to improve detection the outliers \n",
    "    cleaner.fit(\n",
    "        data=train_data, \n",
    "        target_columns=target_columns,\n",
    "        ci_calibration_size=0.1,\n",
    "        #refit_full=True, in our class ConformalAutoGluonCleaner, the refit_full is already set to True, so we do not need to set it again. It will retrain the best model on the dataset, improving s the accuracy and also correcting data.\n",
    "        fit_weighted_ensemble = True, # this parameter was set to False in the class, but we have changed it to True to improve the performance (maximize predictive quality.), yet it will be slower\n",
    "        \n",
    "        ### Bagging/Stacking settings ###\n",
    "        # was set to False, but i changed it to True, to assist in enhancing the precision of the model and reducing overfitting \n",
    "        auto_stack=True, \n",
    "        num_bag_folds=5, #it helps detect and prevent Overfitting and provides a more stable and less biased estimate of how the model will perform.\n",
    "        num_stack_levels=1,  #level 1 to maximize predictive performance     \n",
    "       \n",
    "        ci_ag_predictor_params={\"path_prefix\": \"AutogluonModels\",\n",
    "                                #\"eval_metric\": \"pinball_loss\",                           \n",
    "                               },\n",
    "        #I have used 5 iterations with enough mount of time to allow the model to take the full advantage of training and also to improve it's performance.\n",
    "        ci_ag_fit_params={\"time_limit\": time_limit,\n",
    "                         \"hyperparameter_tune_kwargs\": {\"num_trials\": 5},\n",
    "                         \n",
    "                          \n",
    "                         },\n",
    "    )\n",
    "\n",
    "    cleaned_data, cleaned_mask, prediction_sets = cleaner.transform(\n",
    "                                                    corrupted_data,\n",
    "                                                    reuse_intermediate = reuse_intermediate, \n",
    "                                                  )\n",
    "        \n",
    "    try: \n",
    "        cleaned_data.to_csv(\"Data/cleaned_data.csv\", index=False)\n",
    "        cleaned_mask.to_csv(\"Data/cleaned_mask.csv\", index=False)\n",
    "        print(\"Data has been saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "### Step 4: Calculate the Accuracy for corrected values\n",
    "### in this section, we are comparing the real result with the cleaned data, so we measured the accuracy, precision, recall, and F1 scores\n",
    "### The column education-num has a higher score because its Functional dependency (education (determinant) -> education-num (dependent))\n",
    "### for the FDs education -> education-num, it reconstructs the target values perfectly.\n",
    "    print('####### Classification report for Correction #######')\n",
    "\n",
    "    for col in target_columns:\n",
    "        y_true = y_test[col]\n",
    "        y_pred = cleaned_data[col]\n",
    "        \n",
    "        if col not in regression_columns:\n",
    "            print(\"Classification report: \")\n",
    "            print(col)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "\n",
    "        else:\n",
    "            print(\"Regression report: \")\n",
    "            print(col)\n",
    "            print(f\"MSE: {mean_squared_error(y_true, y_pred)}\")\n",
    "            print(f\"MAE: {mean_absolute_error(y_true,y_pred)}\")\n",
    "            print(f\"RMSE: {root_mean_squared_error(y_true,y_pred)}\")\n",
    "            print('---------------------------')\n",
    "    # print(\"Classification report: \")\n",
    "    # for x in categorical_columns:\n",
    "    #     y_true = y_test[x]\n",
    "    #     y_pred = cleaned_data[x]\n",
    "    #     print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # print(\"Regression report: \")\n",
    "    # for x in regression_columns:\n",
    "    #     y_true = y_test[x]\n",
    "    #     y_pred = cleaned_data[x]\n",
    "    #     print(x)\n",
    "    #     print(f\"MSE: {mean_squared_error(y_true, y_pred)}\")\n",
    "    #     print(f\"MAE: {mean_absolute_error(y_true,y_pred)}\")\n",
    "    #     print(f\"RMSE: {root_mean_squared_error(y_true,y_pred)}\")\n",
    "    #     print('---------------------------')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## Deticting-Part ##\n",
    "    print('####### Detection Part #######')\n",
    "\n",
    "    corrupted = train_data.copy()\n",
    "\n",
    "    corrupted_data, error_mask = create_errors(corrupted, target_columns, condition_columns, error_type, error_rate)\n",
    "    \n",
    "    corrupted_data.to_csv('Data/corrupted_data_E.csv', index=False)\n",
    "    error_mask.to_csv('Data/ground_truth_E.csv', index=False)\n",
    "    \n",
    "    error_mask_Ex1_train=error_mask.copy()\n",
    "    mask_numeric_train = error_mask_Ex1_train.astype(int)\n",
    "    for target in target_columns:\n",
    "        safe_col = target.replace('-', '_').replace(' ', '_')\n",
    "        globals()[f'error_mask_{safe_col}'] = mask_numeric_train[target]\n",
    "        corrupted_data[target + '-mask']=globals()[f'error_mask_{safe_col}'].values\n",
    "    \n",
    "    train_corrupted_data = corrupted_data\n",
    "    cleaned_results={}\n",
    "    final_cleaned_data = corrupted_test_copy.copy()\n",
    "    \n",
    "    for mask in mask_targets: \n",
    "        print(f\"{mask}\")\n",
    "        cleaner = ConformalAutoGluonCleaner(confidence_level=confidence_level,seed=42)\n",
    "        \n",
    "        cleaning=cleaner.fit(\n",
    "            data=train_corrupted_data, \n",
    "            target_columns=[mask],  \n",
    "            ci_calibration_size=0.1,\n",
    "            categorical_precision_threshold=0.7,  \n",
    "            numeric_error_percentile=0.95,      \n",
    "            ci_ag_fit_params={\"time_limit\": time_limit,\n",
    "                              \"hyperparameter_tune_kwargs\": {\"num_trials\": 20},\n",
    "                             },\n",
    "           \n",
    "            )\n",
    "        \n",
    "        cleaned_data, cleaned_mask, prediction_sets = cleaning.transform(corrupted_test_copy)\n",
    "        cleaned_results[mask]={\n",
    "            'cleaned_data': cleaned_data,\n",
    "            'cleaned_mask': cleaned_mask,\n",
    "            'Prediction_sets': prediction_sets\n",
    "        }\n",
    "        \n",
    "        if mask in cleaned_data.columns:\n",
    "            final_cleaned_data[mask] = cleaned_data[mask]\n",
    "        \n",
    "        try: \n",
    "            final_cleaned_data.to_csv(\"Data/cleaned_dataE.csv\", index=False)\n",
    "            print(\"Data has been saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    ## Classification-report for Detection ##\n",
    "    print('####### Classification report for Detection #######')\n",
    "\n",
    "    for target, label in zip(target_columns, mask_targets):\n",
    "    # for i, label in enumerate(mask_targets):\n",
    "        model_path = \"AutogluonModels/\" + label\n",
    "        model = TabularPredictor.load(model_path)\n",
    "        preds = model.predict(corrupted_test_copy)\n",
    "        y_true=masks[target]\n",
    "        print(f\"{label}\")\n",
    "        print(classification_report(y_true, preds))\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "    ## XAI EXplaining Diagramm ##\n",
    "    #source:https://auto.gluon.ai/dev/tutorials/eda/components/autogluon.eda.explain.html\n",
    "    print('####### XAI Explaining Diagramm #######')\n",
    "\n",
    "    train_corrupted_copy = train_corrupted_data.copy()\n",
    "    test_copy =corrupted_test_copy.copy()\n",
    "    \n",
    "    for target in mask_targets:\n",
    "        model_path = \"AutogluonModels/\" + target\n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        \n",
    "        preds = predictor.predict(corrupted_test_copy)\n",
    "        \n",
    "        X_train = train_corrupted_copy.drop(target, axis=1)\n",
    "        X_test=test_copy.drop(target, axis=1)\n",
    "       \n",
    "        rows_to_explain=X_test[preds==1][X_train.columns].head(diagramm_xai_rows)\n",
    "        print(f\"{target}\")\n",
    "    \n",
    "        auto.analyze(\n",
    "            train_data=X_train, model=predictor,\n",
    "            anlz_facets=[\n",
    "                eda.explain.ShapAnalysis(rows_to_explain),\n",
    "            ],\n",
    "            viz_facets=[\n",
    "                viz.explain.ExplainWaterfallPlot(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    print('####### XAI Classification-report for condition columns #######')\n",
    "\n",
    "    ## XAI Classification-report for condition columns ##\n",
    "    #source:https://auto.gluon.ai/0.8.2/_modules/autogluon/eda/analysis/explain.html\n",
    "\n",
    "    # In the report it will show only the precentage of the condition columns\n",
    "    \n",
    "    reports = {}\n",
    "    \n",
    "    top_k = top_k \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    \n",
    "    for target, cond in zip(mask_targets, condition_columns):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        model_path = \"AutogluonModels/\" + target\n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        \n",
    "        y_preds = predictor.predict(corrupted_test_copy)\n",
    "        \n",
    "        X_train = train_corrupted_copy.drop(target, axis=1)\n",
    "        X_test=test_copy.drop(target, axis=1)\n",
    "        \n",
    "        rows=X_test[y_preds==1][X_train.columns].head(rows_numbers_xai)\n",
    "    \n",
    "    \n",
    "        shap_analysis = eda.explain.ShapAnalysis(rows)\n",
    "        auto.analyze(\n",
    "            train_data=X_train,\n",
    "            model=predictor,\n",
    "            anlz_facets=[shap_analysis]\n",
    "        )\n",
    "        shap_results = shap_analysis.state.explain[\"shapley\"]\n",
    "    \n",
    "        for result in shap_results:\n",
    "            shap_values = result.shap_values  \n",
    "            feature_names = result.features.index.tolist()       \n",
    "            top_indices = np.argsort(np.abs(shap_values))[-top_k:][::-1] \n",
    "            top_features = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "            for x in top_features:\n",
    "                y_pred.append(x)\n",
    "                y_true.append(cond)\n",
    "        \n",
    "        counts = Counter(y_pred)   \n",
    "        total = sum(counts.values())\n",
    "        le.fit(y_true + y_pred)\n",
    "        y_true_enc = le.transform(y_true)\n",
    "        y_pred_enc = le.transform(y_pred)\n",
    "        reports[target] = classification_report(y_true_enc, y_pred_enc, target_names=le.classes_, zero_division=0)\n",
    "        print(f\"{target}\")\n",
    "        print('---------------------------------------------------------')\n",
    "        print(f\"Top Features percentage for {target}\")\n",
    "        for feature, cnt in counts.items():\n",
    "            print(f\"{feature}: {cnt/total*100:.2f}%\")\n",
    "        print('---------------------------------------------------------')\n",
    "        \n",
    "    for target, report in reports.items():\n",
    "        print(f\"{target}\")\n",
    "        print(report)\n",
    "        print('---------------------------------------------------------')\n",
    "\n",
    "        \n",
    "\n",
    "def create_errors(cleaned_data, target_columns, condition_columns, error_type, error_rate): \n",
    "    \n",
    "    all_error_models = {} # we will store first our data here, to prevent it from overwriting, and then we will call it in config\n",
    "    for target, cond, err in zip(target_columns, condition_columns, error_rate):\n",
    "        \n",
    "        all_error_models[target] = []\n",
    "        error_model=ErrorModel(\n",
    "                    error_mechanism=error_mechanism.EAR(condition_to_column=cond ,seed=42),\n",
    "                    error_type=error_type,\n",
    "                    error_rate=err\n",
    "                )\n",
    "        all_error_models[target].append(error_model)\n",
    "\n",
    "    config = MidLevelConfig(all_error_models)\n",
    "    corrupted_data, error_mask= mid_level.create_errors(data=cleaned_data, config=config)\n",
    "             \n",
    "    return corrupted_data, error_mask\n",
    "   \n",
    "\n",
    "def export_files(corrupted_data,error_mask):\n",
    "\n",
    "    try: \n",
    "        corrupted_data.to_csv('Data/corrupted_data.csv', index=False)\n",
    "        error_mask.to_csv('Data/ground_truth.csv', index=False)\n",
    "        print(\"Data are now corrupted\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f'Something went wrong: {e}')\n",
    "\n",
    "\n",
    "\n",
    "# Columns = ['age', 'occupation', 'fnlwgt','hours-per-week', 'sex', 'workclass', 'education', 'education-num'] \n",
    "# in this list i have defined 3 types of errors to be choosen -> [error_type.Outlier(), error_type.CategorySwap(), error_type.MissingValue()]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_exp( Dataframe = 'adult',\n",
    "    target_columns = ['sex', 'education', 'workclass', 'occupation'],\n",
    "    condition_columns = ['relationship','education-num','age','hours-per-week'],\n",
    "    # categorical columns ['age', 'hours-per-week','occupation', 'sex', 'workclass', 'education', 'material-status', 'relationship']\n",
    "    categorical_columns = ['sex', 'education', 'workclass', 'occupation'], # for error_type.CategorySwap()\n",
    "    regression_columns = [],  # numeric columns : ['fnlwgt']\n",
    "    error_type = error_type.CategorySwap(), # apply error type Missing value to a single column at a time, to avoid information loss and unstable performance \n",
    "    error_rate = [0.2, 0.3, 0.4, 0.5],\n",
    "    frac = 0.4, # Precent of rows that will be used in the CDC expirement\n",
    "    test_size = 0.3, # Percenet of test data\n",
    "    reuse_intermediate = False, # it's recommended to set it to False when type_error is Missing_Value, otherwise we can set it to True in other type errors to reuse the values we've already predicted instead of starting from square one\n",
    "    rows_numbers_xai = 200, #number of rows to be explained in XAI report section\n",
    "    diagramm_xai_rows = 5,\n",
    "    confidence_level = 0.99,\n",
    "    time_limit = 3600,\n",
    "    top_k = 2\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9cf3d-a24b-4c02-a638-96bbc34f73e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (CDC)",
   "language": "python",
   "name": "cdc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
